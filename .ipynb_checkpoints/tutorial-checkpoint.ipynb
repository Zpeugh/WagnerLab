{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-19ad0a3997b1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-19ad0a3997b1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python setup.py\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import load_data as ld\n",
    "import dataset_utilities as du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some preprocessed data from the 2010 movie dataset.  The load_data module uses multiple methods from the fmri_preprocessing module plus parallelization accross subjects to quickly load in the 2010 Wagner movie dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n"
     ]
    }
   ],
   "source": [
    "ds_list = ld.get_2010_preprocessed_data(num_subjects=5, mask_path='masks/aal_l_hippocampus_3x3x3.nii', num_threads=5, combine=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there are some pesky warnings that I could not for the life of me get to go away. Ignorning these, we can take a look at what we got from importing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered Datasets for 5 subjects\n",
      "Shape of each Dataset: (633, 318)\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathered Datasets for {0} subjects\".format(len(ds_list)))\n",
    "print(\"Shape of each Dataset: {0}\".format(ds_list[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a list of 5 subjects Dataset objects.  Each Dataset contains the preprocessed and spliced runs with 633 time points and 318 voxels inside of the left hippocampus mask we supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take just one of the Datasets and examine the time series for the voxel 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-823205fd36bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoxel_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_list' is not defined"
     ]
    }
   ],
   "source": [
    "ds = ds_list[0]\n",
    "du.voxel_plot(ds, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This voxel_plot() function just quickly lets us examine the time series for a single voxel.  You can see that the runs of have been detrended and z-scored.  Now lets do something a little more interesting.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searchlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run a searchlight analysis between multiple subjects, we need to combine the datasets into one single\n",
    "Dataset object to feed into pymvpa's searchlight function.  To do this we have two options:\n",
    "\n",
    "1: You already knew from the start that you wanted the datasets combined, so in multithreaded we set the combine keyword to True (which it is by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n"
     ]
    }
   ],
   "source": [
    "combined_ds = ld.get_2010_preprocessed_data(num_subjects=5, mask_path='masks/aal_l_hippocampus_3x3x3.nii', num_threads=5, combine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: We take the dataset list we got earlier and use the following function to combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cds = du.combine_datasets(ds_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way we get the same single Dataset object that is ready to be passed to a searchlight method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 318, 633)\n",
      "(5, 318, 633)\n"
     ]
    }
   ],
   "source": [
    "print(combined_ds.shape)\n",
    "print(cds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a simple Pearson's correlation first, with single voxel univariate comparisons.  We do this by setting the\n",
    "searchlight radius to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mvpa2.datasets.base.Dataset'>\n",
      "(1, 318)\n"
     ]
    }
   ],
   "source": [
    "results = du.run_searchlight(cds, metric='correlation', radius=0, n_cpu=4)\n",
    "print(results.__class__)\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we got a Dataset object with 318 average Pearson's correlation values for all pairwise combinations of subjects.  From here we can easily export the data to a nifti file and look at the results in AFNI or another fmri viewing tool.  A simple function to do this is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#du.export_to_nifti(results, 'left_hippo_corr_r0')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to actually make use of the searchlight functionality we can increase the radius.  This will take the regional average in each searchlight and compare each subjects timeseries, and return an average at each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 318)\n"
     ]
    }
   ],
   "source": [
    "corr_res = du.run_searchlight(cds, metric='correlation', radius=2, n_cpu=4)\n",
    "print(corr_res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the measurement to canonical correlation analysis by simply using metric='cca'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cca_res = du.run_searchlight(cds, metric='cca', radius=2, n_cpu=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get a quick look at the how these two results compare in a graph by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "du.plot_isc_vs_isi(corr_res, cca_res, 'Example Title', save=False, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
