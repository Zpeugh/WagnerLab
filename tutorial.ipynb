{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import load_data as ld\n",
    "import dataset_utilities as du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some preprocessed data from the 2010 movie dataset.  The multithreaded module only contains one function to call, and it is a convenience method to parallel preprocess the 2010 data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n"
     ]
    }
   ],
   "source": [
    "ds_list = ld.get_2010_preprocessed_data(num_subjects=5, mask_path='masks/aal_l_hippocampus_3x3x3.nii', num_threads=5, combine=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at what we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered Datasets for 5 subjects\n",
      "Shape of each Dataset: (633, 318)\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathered Datasets for {0} subjects\".format(len(ds_list)))\n",
    "print(\"Shape of each Dataset: {0}\".format(ds_list[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a list of 5 subjects Dataset objects.  Each Dataset contains the preprocessed and spliced runs with 633 time points and 318 voxels inside of the left hippocampus mask we supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take just one of the Datasets and examine the time series for the voxel 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = ds_list[0]\n",
    "du.voxel_plot(ds, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This voxel_plot() function just quickly lets us examine the time series for a single voxel.  You can see that the runs of have been detrended and z-scored.  Now lets do something a little more interesting.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searchlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run a searchlight analysis between multiple subjects, we need to combine the datasets into one single\n",
    "Dataset object to feed into pymvpa's searchlight function.  To do this we have two options:\n",
    "\n",
    "1: You already knew from the start that you wanted the datasets combined, so in multithreaded we set the combine keyword to True (which it is by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n"
     ]
    }
   ],
   "source": [
    "combined_ds = ld.get_2010_preprocessed_data(num_subjects=5, mask_path='masks/aal_l_hippocampus_3x3x3.nii', num_threads=5, combine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: We take the dataset list we got earlier and use the following function to combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cds = du.combine_datasets(ds_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way we get the same single Dataset object that is ready to be passed to a searchlight method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 318, 633)\n",
      "(5, 318, 633)\n"
     ]
    }
   ],
   "source": [
    "print(combined_ds.shape)\n",
    "print(cds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a simple Pearson's correlation first, with single voxel univariate comparisons.  We do this by setting the\n",
    "searchlight radius to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mvpa2.datasets.base.Dataset'>\n",
      "(1, 318)\n"
     ]
    }
   ],
   "source": [
    "results = du.run_searchlight(cds, metric='correlation', radius=0, n_cpu=4)\n",
    "print(results.__class__)\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we got a Dataset object with 318 average Pearson's correlation values for all pairwise combinations of subjects.  From here we can easily export the data to a nifti file and look at the results in AFNI or another fmri viewing tool.  A simple function to do this is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#du.export_to_nifti(results, 'left_hippo_corr_r0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
